# TurboMeta - RayBan Meta Smart Glasses AI Assistant

<div align="center">

![TurboMeta Logo](./rayban.png)

**ğŸŒ World's First Full-Chinese AI Multimodal RayBan Meta Assistant**

[![Platform](https://img.shields.io/badge/Platform-iOS%2017.0%2B-blue.svg)](https://www.apple.com/ios/)
[![Swift](https://img.shields.io/badge/Swift-5.0-orange.svg)](https://swift.org)
[![License](https://img.shields.io/badge/License-Meta-green.svg)](LICENSE)

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README_CN.md)

</div>

## ğŸ“– Introduction

TurboMeta is a full-featured multimodal AI assistant built exclusively for RayBan Meta smart glasses, powered by Alibaba Cloud's Qwen multimodal AI models:

- ğŸ¯ **Live AI Conversations**: Real-time multimodal interaction through glasses camera and microphone
- ğŸ¬ **Walk Into Movie**: Capture a scene and get a cinematic reference plus atmospheric narration
- ğŸ‘ï¸ **Image Recognition**: Intelligently identify objects, scenes, and text in your field of view
- ğŸ¥ **Live Streaming**: Stream directly to platforms like Douyin, Kuaishou, and Xiaohongshu
- ğŸŒ **Full Chinese Support**: Complete Chinese AI interaction experience, perfectly tailored for Chinese users

This is the world's first **fully Chinese-enabled** RayBan Meta AI assistant, bringing the convenience of smart glasses to Chinese-speaking users.

## âœ¨ Core Features

### ğŸ¤– Live AI - Real-time Conversations
- **Multimodal Interaction**: Simultaneous voice and visual input support
- **Real-time Response**: Based on Qwen Omni-Realtime model with low-latency voice conversations
- **Scene Understanding**: AI can see what's in front of you and provide relevant suggestions
- **Natural Responses**: Smooth and natural Chinese conversation experience
- **One-tap Hide**: Support for hiding conversation interface to focus on visual experience

### ğŸ¬ Walk Into Movie - Cinematic Atmosphere
- **Auto Capture**: Starts a high-quality stream and grabs a frame after entry
- **Scene Match**: Picks the movie/series/game vibe that fits the scene
- **Atmosphere Narration**: One-line hook plus cinematic voiceover
- **Voice Output**: Plays the narration aloud

### ğŸ“¸ Real-time Photography
- **Auto-start**: Automatically connects to glasses and starts preview when opened
- **Multi-function Integration**: Choose AI recognition after taking photos
- **Smooth Experience**: Real-time video stream preview

### ğŸ¥ Live Streaming
- **Platform Support**: Compatible with mainstream live streaming platforms
- **Clean Interface**: Pure view focused on streaming content

## ğŸ› ï¸ Tech Stack

- **Platform**: iOS 17.0+
- **Language**: Swift 5.0 + SwiftUI
- **SDK**: Meta Wearables DAT SDK v0.3.0
- **AI Models**:
  - Qwen Omni-Realtime: Real-time multimodal conversations
  - Qwen VL-Plus: Visual understanding and image analysis
- **Architecture**: MVVM + Combine
- **Audio**: AVAudioEngine + AVAudioPlayerNode

## ğŸ“‹ Requirements

### Hardware Requirements
- âœ… RayBan Meta Smart Glasses (Stories or latest model)
- âœ… iPhone (iOS 17.0 or higher)
- âœ… Stable internet connection

### Software Requirements
- âœ… Xcode 15.0 or higher
- âœ… Meta View App (for pairing glasses)
- âœ… Alibaba Cloud account (for API access)

### API Requirements
You need to apply for the following Alibaba Cloud APIs:
1. **Qwen Omni-Realtime API**: For real-time conversations
2. **Qwen VL-Plus API**: For image recognition and Walk Into Movie

ğŸ‘‰ [Apply for APIs at Alibaba Cloud](https://dashscope.aliyun.com/)

## ğŸš€ Installation Guide

### Step 1: Enable RayBan Meta Developer Mode

âš ï¸ **Important**: Since this is currently in Preview phase, you must enable developer mode to use it.

1. Open **Meta View App** (or **Meta AI App**) on your iPhone
2. Go to **Settings** â†’ **App Info** or **About**
3. Find **Version Number**
4. **Tap the version number 5 times consecutively**
5. You'll see a "Developer mode enabled" message

### Step 2: Configure API Key

1. Go to [Alibaba Cloud DashScope](https://dashscope.aliyun.com/)
2. Log in and create an API Key
3. Open `VisionAPIConfig.swift` in the project
4. Replace with your API Key:

```swift
struct VisionAPIConfig {
    static let apiKey = "sk-YOUR-API-KEY-HERE"
}
```

### Step 3: Build the Project

1. Open `CameraAccess.xcodeproj` with Xcode
2. Select your development team (Team)
3. Modify Bundle Identifier (if needed)
4. Connect your iPhone
5. Click **Run** or press `Cmd + R`

### Step 4: Signing and Installation

#### Method A: Direct Installation with Xcode (Recommended)
1. Select your iPhone device in Xcode
2. Click the Run button
3. First-time run requires trusting the developer in iPhone Settings

#### Method B: Export IPA and Self-Sign
1. In Xcode, select **Product** â†’ **Archive**
2. Export the IPA file
3. Use AltStore, Sideloadly, or other tools to sign and install

```bash
# Using ios-deploy (requires installation)
brew install ios-deploy
ios-deploy --bundle YourApp.app
```

### Step 5: Pair Your Glasses

1. Open Meta View App
2. Pair your RayBan Meta glasses
3. Ensure Bluetooth is enabled
4. Return to TurboMeta App and wait for connection success

## ğŸ“± Usage Guide

### First-time Use

1. Launch TurboMeta App
2. Ensure RayBan Meta glasses are paired and turned on
3. Wait for device connection (status shown at top)
4. Select the feature you want to use

### Live AI Real-time Conversations

1. Tap the **Live AI** card on the home screen
2. Wait for connection success (green dot in upper right)
3. Start speaking, AI will respond in real-time
4. AI can see what's in front of you
5. Tap the ğŸ‘ï¸ button to hide conversation history

**Tips**:
- Speak clearly and maintain appropriate distance
- Ask "What do you see?" to have AI describe the scene
- AI responds in concise Chinese

### Walk Into Movie

1. Tap the **Walk Into Movie** card on the home screen
2. The app starts a high-quality stream
3. After 1 second, it captures a frame and analyzes the scene
4. Listen to the cinematic one-liner and atmosphere narration
5. Tap **Try Again** to generate a new result

**Use Cases**:
- Generate a cinematic vibe for street walks
- Add atmosphere to travel moments
- Turn everyday scenes into story hooks

### Live Streaming

1. Tap the **Live Stream** card on the home screen
2. Wait for video stream to start
3. Create your streaming content
4. Tap stop button to end the stream

## ğŸ¨ Interface Preview

<table>
  <tr>
    <td><b>Home</b></td>
    <td><b>Live AI</b></td>
    <td><b>Walk Into Movie</b></td>
  </tr>
  <tr>
    <td><img src="./screenshots/home.png" width="200"/></td>
    <td><img src="./screenshots/liveai.png" width="200"/></td>
    <td><img src="./screenshots/nutrition.png" width="200"/></td>
  </tr>
</table>

## âš™ï¸ Configuration Options

### API Configuration

Configure in `VisionAPIConfig.swift`:

```swift
struct VisionAPIConfig {
    // Alibaba Cloud API Key
    static let apiKey = "sk-YOUR-API-KEY-HERE"

    // API Base URL (usually doesn't need modification)
    static let baseURL = "https://dashscope.aliyuncs.com"
}
```

### System Prompts

Customize AI response style in `OmniRealtimeService.swift`:

```swift
"instructions": "You are a RayBan Meta smart glasses AI assistant. Keep answers concise and conversational..."
```

## ğŸ”§ Troubleshooting

### Q1: Glasses won't connect?

**Solutions**:
1. Ensure glasses are successfully paired in Meta View App
2. Check if Bluetooth is enabled
3. Restart TurboMeta App
4. Restart glasses (place in charging case)
5. Ensure developer mode is enabled

### Q2: AI not responding or responding slowly?

**Solutions**:
1. Check if internet connection is stable
2. Verify API Key is correctly configured
3. Check if Alibaba Cloud API quota is sufficient
4. Review console logs for errors

### Q3: Walk Into Movie results feel off?

**Solutions**:
1. Keep the frame steady and clear
2. Capture in good lighting
3. Include enough environment context
4. Results are for entertainment only

### Q4: Cannot install on phone?

**Solutions**:
1. Check if iPhone is in trusted devices list
2. Verify developer certificate is valid
3. Modify Bundle Identifier to avoid conflicts
4. Free Apple Developer accounts require re-signing every 7 days

### Q5: Voice recognition inaccurate?

**Solutions**:
1. Ensure environment is relatively quiet
2. Speak clearly at moderate speed
3. Don't obstruct the microphone
4. Currently optimized for Chinese, other languages may be less accurate

## ğŸ”’ Privacy and Security

- âœ… All audio/video data is only used for AI processing
- âœ… No storage or upload of user privacy data
- âœ… API communications use HTTPS encryption
- âœ… Images and voice are retained only during session
- âœ… Complies with Apple and Meta privacy policies

## ğŸ—ºï¸ Roadmap

### âœ… Completed
- [x] Live AI real-time conversations
- [x] Walk Into Movie
- [x] Image recognition
- [x] Basic live streaming functionality
- [x] Bilingual Chinese/English support
- [x] Conversation history saving
- [x] One-tap hide conversations

### ğŸš§ In Progress
- [ ] Improve multilingual support
- [ ] Optimize UI/UX
- [ ] Performance optimization

### ğŸ“… Planned
- [ ] Real-time translation feature
- [ ] WordLearn vocabulary learning
- [ ] Cloud conversation sync
- [ ] More live streaming platform support
- [ ] Offline mode
- [ ] Apple Watch companion app

## ğŸ¤ Contributing

Contributions, bug reports, and feature suggestions are welcome!

1. Fork this project
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is based on modifications of original code from Meta Platforms, Inc. and follows the original project's license.

Some code copyright belongs to Meta Platforms, Inc. and its affiliates.

Please see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Meta Platforms, Inc.** - For providing DAT SDK and original sample code
- **Alibaba Cloud Qwen Team** - For powerful multimodal AI capabilities
- **RayBan** - For excellent smart glasses hardware

## ğŸš€ How to Open Source This Project

### 1. Create GitHub Repository

```bash
# Create a new repository on GitHub website
# Then execute in your project directory:
git init
git add .
git commit -m "Initial commit: TurboMeta - RayBan Meta AI Assistant"
git branch -M main
git remote add origin https://github.com/your-username/your-repo.git
git push -u origin main
```

### 2. Protect Sensitive Information

âœ… **Security Measures Implemented**:
- API Keys are no longer hardcoded in the source code
- Uses iOS Keychain for secure storage of user API Keys
- Users configure their own API Keys in the App Settings

âš ï¸ **Pre-release Checklist**:
```bash
# Search for potential sensitive information
grep -r "sk-" .
grep -r "API" . | grep -i "key"
```

### 3. Add .gitignore File

Create a `.gitignore` file in project root:

```gitignore
# Xcode
build/
*.pbxuser
*.mode1v3
*.mode2v3
*.perspectivev3
xcuserdata/
*.xccheckout
*.moved-aside
DerivedData/
*.hmap
*.ipa
*.xcuserstate
*.xcworkspace

# API Keys (extra protection)
**/*APIKey*.swift
**/APIKeys.swift
**/*Secret*.swift

# macOS
.DS_Store
```

### 4. Choose Open Source License

This project is based on Meta DAT SDK sample code and follows the original project's license. You can:
- Keep the same license as Meta's original project
- Choose MIT, Apache 2.0, or other permissive licenses for your code
- Acknowledge the original source code in the LICENSE file

### 5. User Configuration Instructions

âš ï¸ **Important Notice**: Users of this project need to:

1. **Apply for API Key**: Visit [Alibaba Cloud DashScope](https://dashscope.aliyun.com/)
2. **Configure API Key**: Enter in App Settings â†’ API Key Management
3. **API Key Security**: Stored securely in iOS Keychain, never exposed

## ğŸŒŸ If This Project Helps You

- â­ï¸ Star the project
- ğŸ› Report bugs or suggest features
- ğŸ”€ Fork and contribute code
- ğŸ“¢ Share with others

---

<div align="center">

**Making Smart Glasses Speak Chinese ğŸ‡¨ğŸ‡³**

Made with â¤ï¸ for RayBan Meta Users

</div>
